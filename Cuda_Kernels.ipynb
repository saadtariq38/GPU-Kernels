{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPU Kernels with PyTorch Executions and Debugging"
      ],
      "metadata": {
        "id": "JzBoOh_S7orO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWEkU9kj_qEl",
        "outputId": "e388206b-e73e-47e0-db2a-7af5b9296fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr  7 16:05:09 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Write the CUDA program to a file\n",
        "cat > hello_gpu_print.cu << 'EOF'\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// Define a simple struct to hold thread information.\n",
        "struct ThreadInfo {\n",
        "    int global_id;\n",
        "    int block;   // Block index\n",
        "    int thread;  // Thread index within the block\n",
        "};\n",
        "\n",
        "// Kernel that writes each thread's info into an array.\n",
        "__global__ void helloGPU(ThreadInfo* out, int total_threads) {\n",
        "    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (tid < total_threads) {\n",
        "        out[tid].block = blockIdx.x;\n",
        "        out[tid].thread = threadIdx.x;\n",
        "        out[tid].global_id = tid;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int blocks = 2;\n",
        "    int threadsPerBlock = 5;\n",
        "    int total_threads = blocks * threadsPerBlock;\n",
        "\n",
        "    // Allocate device memory for the output array.\n",
        "    ThreadInfo* d_out;\n",
        "    cudaMalloc((void**)&d_out, total_threads * sizeof(ThreadInfo));\n",
        "\n",
        "    // Launch the kernel.\n",
        "    helloGPU<<<blocks, threadsPerBlock>>>(d_out, total_threads);\n",
        "    // Wait for the GPU to finish execution.\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Allocate host memory and copy the device data to the host.\n",
        "    ThreadInfo* h_out = (ThreadInfo*)malloc(total_threads * sizeof(ThreadInfo));\n",
        "    cudaMemcpy(h_out, d_out, total_threads * sizeof(ThreadInfo), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print the messages from the host.\n",
        "    for (int i = 0; i < total_threads; i++) {\n",
        "        printf(\"Hello GPU from thread %d in block %d with global id %d\\n\", h_out[i].thread, h_out[i].block, h_out[i].global_id);\n",
        "    }\n",
        "\n",
        "    // Free host and device memory.\n",
        "    free(h_out);\n",
        "    cudaFree(d_out);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "EOF\n",
        "\n",
        "# Compile and run the CUDA program. sm_70 architecture needed for printf\n",
        "nvcc -arch=sm_70 hello_gpu_print.cu -o hello_gpu_print && ./hello_gpu_print\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWl4tfLqABhU",
        "outputId": "b16c5609-fc55-4c94-ce60-80ad8af2f00b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello GPU from thread 0 in block 0 with global id 0\n",
            "Hello GPU from thread 1 in block 0 with global id 1\n",
            "Hello GPU from thread 2 in block 0 with global id 2\n",
            "Hello GPU from thread 3 in block 0 with global id 3\n",
            "Hello GPU from thread 4 in block 0 with global id 4\n",
            "Hello GPU from thread 0 in block 1 with global id 5\n",
            "Hello GPU from thread 1 in block 1 with global id 6\n",
            "Hello GPU from thread 2 in block 1 with global id 7\n",
            "Hello GPU from thread 3 in block 1 with global id 8\n",
            "Hello GPU from thread 4 in block 1 with global id 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > matmul.cu << 'EOF'\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <math.h>\n",
        "#include <chrono>\n",
        "\n",
        "// Macro to convert 2D indices into a 1D index.\n",
        "#define IDX(row, col, n) ((row)*(n) + (col))\n",
        "\n",
        "__global__ void matrixMulKernel(float* A, float* B, float* C, int n) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Ensure that we are within bounds in case n is not a multiple of the block size.\n",
        "    if (row < n && col < n) {\n",
        "        float sum = 0.0f;\n",
        "        for (int k = 0; k < n; k++) {\n",
        "            sum += A[IDX(row, k, n)] * B[IDX(k, col, n)];\n",
        "        }\n",
        "        C[IDX(row, col, n)] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "void matrixMulCPU(const float* A, const float* B, float* C, int n) {\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < n; j++) {\n",
        "            float sum = 0.0f;\n",
        "            for (int k = 0; k < n; k++) {\n",
        "                sum += A[i * n + k] * B[k * n + j];\n",
        "            }\n",
        "            C[i * n + j] = sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "bool compareMatrices(const float* ref, const float* data, int n, float tol = 1e-3f) {\n",
        "    for (int i = 0; i < n * n; i++) {\n",
        "        if (fabs(ref[i] - data[i]) > tol) {\n",
        "            return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 512;\n",
        "    size_t bytes = n * n * sizeof(float);\n",
        "\n",
        "    float *h_A = (float*)malloc(bytes);\n",
        "    float *h_B = (float*)malloc(bytes);\n",
        "    float *h_C = (float*)malloc(bytes);\n",
        "    float *h_C_cpu = (float*)malloc(bytes);\n",
        "    for (int i = 0; i < n * n; i++) {\n",
        "        h_A[i] = (float)rand() / RAND_MAX;\n",
        "        h_B[i] = (float)rand() / RAND_MAX;\n",
        "    }\n",
        "\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    cudaMalloc((void**)&d_A, bytes);\n",
        "    cudaMalloc((void**)&d_B, bytes);\n",
        "    cudaMalloc((void**)&d_C, bytes);\n",
        "\n",
        "    cudaMemcpy(d_A, h_A, bytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 blockDim(16, 16);\n",
        "    dim3 gridDim((n + blockDim.x - 1) / blockDim.x,\n",
        "                 (n + blockDim.y - 1) / blockDim.y);\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    matrixMulKernel<<<gridDim, blockDim>>>(d_A, d_B, d_C, n);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"CUDA kernel launch error: %s\\n\", cudaGetErrorString(err));\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "\n",
        "    float elapsedTime;\n",
        "    cudaEventElapsedTime(&elapsedTime, start, stop);\n",
        "    printf(\"GPU kernel execution time: %f ms\\n\", elapsedTime);\n",
        "\n",
        "    cudaMemcpy(h_C, d_C, bytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    auto cpu_start = std::chrono::high_resolution_clock::now();\n",
        "    matrixMulCPU(h_A, h_B, h_C_cpu, n);\n",
        "    auto cpu_end = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double, std::milli> cpu_duration = cpu_end - cpu_start;\n",
        "    printf(\"CPU computation time: %f ms\\n\", cpu_duration.count());\n",
        "\n",
        "    if (compareMatrices(h_C_cpu, h_C, n)) {\n",
        "        printf(\"Results match!\\n\");\n",
        "    } else {\n",
        "        printf(\"Results differ!\\n\");\n",
        "    }\n",
        "\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "    free(h_C_cpu);\n",
        "\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "EOF\n",
        "\n",
        "# Compile the CUDA program.\n",
        "nvcc -arch=sm_70 matmul.cu -o matmul && ./matmul\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArDNhU2zGZfw",
        "outputId": "86bb05ac-8aac-4d2d-c678-217d1a43b7d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU kernel execution time: 1.300832 ms\n",
            "CPU computation time: 804.479288 ms\n",
            "Results match!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > matmul.cu << 'EOF'\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <math.h>\n",
        "#include <chrono>\n",
        "\n",
        "// Macro to convert 2D indices into a 1D index.\n",
        "#define IDX(row, col, n) ((row)*(n) + (col))\n",
        "\n",
        "// Define TILE_WIDTH as a constant. Here we choose 16.\n",
        "#define TILE_WIDTH 16\n",
        "\n",
        "// GPU Kernel: Tiled Matrix Multiplication using Shared Memory.\n",
        "__global__ void matrixMulShared(float* A, float* B, float* C, int n) {\n",
        "    // 1. Compute the global row and column index for each thread.\n",
        "    int row = blockIdx.y * TILE_WIDTH + threadIdx.y;\n",
        "    int col = blockIdx.x * TILE_WIDTH + threadIdx.x;\n",
        "\n",
        "    // 2. Declare shared memory arrays to hold a tile (sub-matrix) from A and from B.\n",
        "    __shared__ float A_tile[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ float B_tile[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    float sum = 0.0f;  // This will accumulate our partial sums.\n",
        "\n",
        "    // 3. Loop over the tiles. The loop runs for ceil(n / TILE_WIDTH) iterations.\n",
        "    for (int t = 0; t < (n + TILE_WIDTH - 1) / TILE_WIDTH; t++) {\n",
        "        // --- Load data into shared memory tiles from global memory ---\n",
        "\n",
        "        // For matrix A:\n",
        "        //    The element of A to be loaded is at row 'row' and column 't * TILE_WIDTH + threadIdx.x'.\n",
        "        //    We must check that these indices are within the bounds of the matrix.\n",
        "        if (row < n && t * TILE_WIDTH + threadIdx.x < n)\n",
        "            A_tile[threadIdx.y][threadIdx.x] = A[row * n + t * TILE_WIDTH + threadIdx.x];\n",
        "        else\n",
        "            A_tile[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "        // For matrix B:\n",
        "        //    The element of B to be loaded is at row 't * TILE_WIDTH + threadIdx.y' and column 'col'.\n",
        "        //    Again, boundary checking is necessary.\n",
        "        if (col < n && t * TILE_WIDTH + threadIdx.y < n)\n",
        "            B_tile[threadIdx.y][threadIdx.x] = B[(t * TILE_WIDTH + threadIdx.y) * n + col];\n",
        "        else\n",
        "            B_tile[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "        // Synchronize threads to ensure the tile is completely loaded.\n",
        "        __syncthreads();\n",
        "\n",
        "        // --- Compute partial sum for the current tile ---\n",
        "        // Each thread calculates the dot product for one output element.\n",
        "        for (int i = 0; i < TILE_WIDTH; i++) {\n",
        "            sum += A_tile[threadIdx.y][i] * B_tile[i][threadIdx.x];\n",
        "        }\n",
        "\n",
        "        // Synchronize threads before loading the next tile.\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // 4. Write the final result to the global result matrix C.\n",
        "    if (row < n && col < n)\n",
        "        C[row * n + col] = sum;\n",
        "}\n",
        "\n",
        "// CPU Version of Matrix Multiplication (for verification and timing).\n",
        "void matrixMulCPU(const float* A, const float* B, float* C, int n) {\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < n; j++) {\n",
        "            float sum = 0.0f;\n",
        "            for (int k = 0; k < n; k++) {\n",
        "                sum += A[i * n + k] * B[k * n + j];\n",
        "            }\n",
        "            C[i * n + j] = sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Compare two matrices element-wise within a tolerance.\n",
        "bool compareMatrices(const float* ref, const float* data, int n, float tol = 1e-3f) {\n",
        "    for (int i = 0; i < n * n; i++) {\n",
        "        if (fabs(ref[i] - data[i]) > tol) {\n",
        "            return false;\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 512;  // Dimension of the matrices (n x n)\n",
        "    size_t bytes = n * n * sizeof(float);\n",
        "\n",
        "    // Allocate host memory for matrices A, B, and result matrices (for both GPU and CPU versions).\n",
        "    float *h_A = (float*)malloc(bytes);\n",
        "    float *h_B = (float*)malloc(bytes);\n",
        "    float *h_C = (float*)malloc(bytes);     // To store GPU result.\n",
        "    float *h_C_cpu = (float*)malloc(bytes);   // To store CPU result.\n",
        "\n",
        "    // Initialize matrices A and B with random values.\n",
        "    for (int i = 0; i < n * n; i++) {\n",
        "        h_A[i] = (float)rand() / RAND_MAX;\n",
        "        h_B[i] = (float)rand() / RAND_MAX;\n",
        "    }\n",
        "\n",
        "    // Allocate device memory.\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    cudaMalloc((void**)&d_A, bytes);\n",
        "    cudaMalloc((void**)&d_B, bytes);\n",
        "    cudaMalloc((void**)&d_C, bytes);\n",
        "\n",
        "    // Copy matrices A and B from host to device.\n",
        "    cudaMemcpy(d_A, h_A, bytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define block and grid dimensions.\n",
        "    // We use a block of size TILE_WIDTH x TILE_WIDTH.\n",
        "    dim3 blockDim(TILE_WIDTH, TILE_WIDTH);\n",
        "    // The grid dimensions are computed to cover an n x n matrix.\n",
        "    dim3 gridDim((n + blockDim.x - 1) / blockDim.x,\n",
        "                 (n + blockDim.y - 1) / blockDim.y);\n",
        "\n",
        "    // Create CUDA events for timing the GPU kernel.\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    // Launch the shared memory matrix multiplication kernel.\n",
        "    matrixMulShared<<<gridDim, blockDim>>>(d_A, d_B, d_C, n);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // Check for any kernel launch errors.\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"CUDA kernel launch error: %s\\n\", cudaGetErrorString(err));\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    // Measure GPU execution time.\n",
        "    float elapsedTime;\n",
        "    cudaEventElapsedTime(&elapsedTime, start, stop);\n",
        "    printf(\"GPU kernel execution time: %f ms\\n\", elapsedTime);\n",
        "\n",
        "    // Copy the result matrix from device to host.\n",
        "    cudaMemcpy(h_C, d_C, bytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Perform matrix multiplication on the CPU for verification and timing.\n",
        "    auto cpu_start = std::chrono::high_resolution_clock::now();\n",
        "    matrixMulCPU(h_A, h_B, h_C_cpu, n);\n",
        "    auto cpu_end = std::chrono::high_resolution_clock::now();\n",
        "    std::chrono::duration<double, std::milli> cpu_duration = cpu_end - cpu_start;\n",
        "    printf(\"CPU computation time: %f ms\\n\", cpu_duration.count());\n",
        "\n",
        "    // Verify that the GPU and CPU results match.\n",
        "    if (compareMatrices(h_C_cpu, h_C, n)) {\n",
        "        printf(\"Results match!\\n\");\n",
        "    } else {\n",
        "        printf(\"Results differ!\\n\");\n",
        "    }\n",
        "\n",
        "    // Free device and host memory.\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "    free(h_C_cpu);\n",
        "\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "EOF\n",
        "\n",
        "# Compile the CUDA program with an appropriate architecture flag and run.\n",
        "nvcc -arch=sm_70 matmul.cu -o matmul && ./matmul\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OsCflNuU-H0",
        "outputId": "5dc6e19a-8752-4705-917c-82710a20f657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU kernel execution time: 0.894240 ms\n",
            "CPU computation time: 1347.606422 ms\n",
            "Results match!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learned memory coalescing and banking conflicts. Implemeneted and tested pytorch profiling"
      ],
      "metadata": {
        "id": "YWn-aKZP4vLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avoiding banking conflicts in shared memory access"
      ],
      "metadata": {
        "id": "SJjV4_n37Uze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CELL NOT MEANT TO BE RUN (ONLY FOR LEARNING)\n",
        "\n",
        "Avoiding Bank Conflicts by Padding\n",
        "One common way to avoid bank conflicts is to add padding to your shared memory arrays so that the stride is not a multiple of the number of banks.\n",
        "\n",
        "Code Example: Bank Conflict-Free Kernel\n",
        "Imagine now you have a 2D shared memory array that represents a tile, and you add an extra column as padding.\n",
        "\n",
        "cpp\n",
        "Copy\n",
        "// Define TILE_WIDTH (assume 32 for this example)\n",
        "#define TILE_WIDTH 32\n",
        "\n",
        "__global__ void bankConflictFreeKernel(float *input, float *output) {\n",
        "    // Declare a 2D shared memory array with padding.\n",
        "    // Note the dimension is [TILE_WIDTH][TILE_WIDTH+1]\n",
        "    // The extra column helps ensure that consecutive rows start at different banks.\n",
        "    __shared__ float sdata[TILE_WIDTH][TILE_WIDTH + 1];\n",
        "\n",
        "    int tid = threadIdx.x; // Overall thread index within a block.\n",
        "    int row = threadIdx.y;\n",
        "    int col = threadIdx.x; // For simplicity, assume a 2D block of size TILE_WIDTH x TILE_WIDTH.\n",
        "\n",
        "    // Each thread loads its element.\n",
        "    // Because of the padding in the second dimension, the address of each element is:\n",
        "    //   Address = row * (TILE_WIDTH + 1) + col\n",
        "    // Now the bank for a given element becomes:\n",
        "    //   (row*(TILE_WIDTH+1) + col) mod number_of_banks.\n",
        "    // With TILE_WIDTH = 32 and an extra 1, the stride is 33 instead of 32.\n",
        "    // This typically avoids the scenario where all accesses fall into the same bank.\n",
        "    if (row < TILE_WIDTH && col < TILE_WIDTH)\n",
        "        sdata[row][col] = input[row * TILE_WIDTH + col];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // For this example, simply write back to global memory.\n",
        "    // Combine row and col to index the 1D global output.\n",
        "    if (row < TILE_WIDTH && col < TILE_WIDTH)\n",
        "        output[row * TILE_WIDTH + col] = sdata[row][col];\n",
        "}\n",
        "Explanation:\n",
        "\n",
        "Here we use a 2D shared array sdata[TILE_WIDTH][TILE_WIDTH+1].\n",
        "\n",
        "The extra column (making the stride 33 instead of 32) changes the mapping:\n",
        "\n",
        "For row = 0, elements are at addresses 0, 1, 2, …, 31\n",
        "\n",
        "For row = 1, elements are at addresses 33, 34, …, 33+31\n",
        "\n",
        "This alteration means that the starting address of each row is offset by 33 mod 32\n",
        "=\n",
        "1\n",
        "=1 relative to the previous row. Consequently, threads in consecutive rows do not access the same bank, thus minimizing potential bank conflicts."
      ],
      "metadata": {
        "id": "nfQoEQhD7Sbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix Multiplcation with Shared Memory"
      ],
      "metadata": {
        "id": "5UH33aAr7W5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > matmul_extension.cpp << 'EOF'\n",
        "#include <torch/extension.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define TILE_WIDTH 16\n",
        "#define IDX(row, col, n) ((row)*(n) + (col))\n",
        "\n",
        "// Our shared memory matrix multiplication kernel.\n",
        "__global__ void matrixMulShared_kernel(const float* A, const float* B, float* C, int n) {\n",
        "    int row = blockIdx.y * TILE_WIDTH + threadIdx.y;\n",
        "    int col = blockIdx.x * TILE_WIDTH + threadIdx.x;\n",
        "\n",
        "    __shared__ float A_tile[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ float B_tile[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    float sum = 0.0f;\n",
        "    for (int t = 0; t < (n + TILE_WIDTH - 1) / TILE_WIDTH; t++) {\n",
        "        if (row < n && t * TILE_WIDTH + threadIdx.x < n)\n",
        "            A_tile[threadIdx.y][threadIdx.x] = A[row * n + t * TILE_WIDTH + threadIdx.x];\n",
        "        else\n",
        "            A_tile[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "        if (col < n && t * TILE_WIDTH + threadIdx.y < n)\n",
        "            B_tile[threadIdx.y][threadIdx.x] = B[(t * TILE_WIDTH + threadIdx.y) * n + col];\n",
        "        else\n",
        "            B_tile[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "        __syncthreads();\n",
        "        for (int i = 0; i < TILE_WIDTH; i++) {\n",
        "            sum += A_tile[threadIdx.y][i] * B_tile[i][threadIdx.x];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (row < n && col < n)\n",
        "        C[row * n + col] = sum;\n",
        "}\n",
        "\n",
        "// Launcher function that wraps kernel launch.\n",
        "void matrixMulShared_launcher(const torch::Tensor A, const torch::Tensor B, torch::Tensor C, int n) {\n",
        "    const int threads = TILE_WIDTH;\n",
        "    dim3 block(threads, threads);\n",
        "    dim3 grid((n + threads - 1) / threads, (n + threads - 1) / threads);\n",
        "    matrixMulShared_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), n);\n",
        "}\n",
        "\n",
        "// Bind the function using pybind11.\n",
        "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
        "    m.def(\"matrix_mul_shared\", &matrixMulShared_launcher, \"Tiled Matrix Multiplication using Shared Memory (CUDA)\");\n",
        "}\n",
        "EOF\n"
      ],
      "metadata": {
        "id": "oY1XGxZL7g1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Activation Function (ClippedReLu) implementation & PyTorch export and comparison"
      ],
      "metadata": {
        "id": "EGSjr4my7B4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > custom_activation.cu <<'EOF'\n",
        "#include <cuda_runtime.h>\n",
        "#include <cstdio>\n",
        "\n",
        "// =====================\n",
        "// device kernels\n",
        "// =====================\n",
        "__global__ void clipped_relu_forward_kernel(const float* x,\n",
        "                                            float* y,\n",
        "                                            float cap,\n",
        "                                            int64_t n)\n",
        "{\n",
        "    int64_t i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < n) {\n",
        "        float v = x[i];\n",
        "        v = v < 0.f ? 0.f : v;\n",
        "        v = v > cap ? cap : v;\n",
        "        y[i] = v;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void clipped_relu_backward_kernel(const float* x,\n",
        "                                             const float* gy,\n",
        "                                             float* gx,\n",
        "                                             float cap,\n",
        "                                             int64_t n)\n",
        "{\n",
        "    int64_t i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < n) {\n",
        "        float g = (x[i] < 0.f || x[i] > cap) ? 0.f : 1.f;\n",
        "        gx[i] = g * gy[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "extern \"C\" void clipped_relu_forward_launcher(const float* x,\n",
        "                                              float* y,\n",
        "                                              float cap,\n",
        "                                              int64_t n,\n",
        "                                              cudaStream_t stream)\n",
        "{\n",
        "    const int TPB = 256;\n",
        "    int blocks = (n + TPB - 1) / TPB;\n",
        "    clipped_relu_forward_kernel<<<blocks, TPB, 0, stream>>>(x, y, cap, n);\n",
        "}\n",
        "\n",
        "extern \"C\" void clipped_relu_backward_launcher(const float* x,\n",
        "                                               const float* gy,\n",
        "                                               float* gx,\n",
        "                                               float cap,\n",
        "                                               int64_t n,\n",
        "                                               cudaStream_t stream)\n",
        "{\n",
        "    const int TPB = 256;\n",
        "    int blocks = (n + TPB - 1) / TPB;\n",
        "    clipped_relu_backward_kernel<<<blocks, TPB, 0, stream>>>(x, gy, gx, cap, n);\n",
        "}\n",
        "EOF\n"
      ],
      "metadata": {
        "id": "g00kRtt8909Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -O3 -shared -Xcompiler -fPIC \\\n",
        "      custom_activation.cu -o custom_activation.so\n"
      ],
      "metadata": {
        "id": "OekqZPU6-Qh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ctypes, torch, math, os\n",
        "\n",
        "# 3.1 load the .so\n",
        "lib = ctypes.CDLL(os.path.abspath(\"custom_activation.so\"))\n",
        "\n",
        "# 3.2 declare argtypes\n",
        "ptr = ctypes.c_void_p\n",
        "c_float = ctypes.c_float\n",
        "c_int64 = ctypes.c_longlong\n",
        "c_stream = ctypes.c_void_p   # cudaStream_t\n",
        "\n",
        "lib.clipped_relu_forward_launcher.argtypes  = [ptr, ptr, c_float, c_int64, c_stream]\n",
        "lib.clipped_relu_backward_launcher.argtypes = [ptr, ptr, ptr, c_float, c_int64, c_stream]\n",
        "# returns are void ⇒ no restype needed\n",
        "\n",
        "# 3.3 autograd Function\n",
        "class ClippedReLU(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, cap: float = 6.0):\n",
        "        x = x.contiguous()\n",
        "        y = torch.empty_like(x)\n",
        "        n = x.numel()\n",
        "        stream_ptr = torch.cuda.current_stream().cuda_stream\n",
        "\n",
        "        lib.clipped_relu_forward_launcher(x.data_ptr(), y.data_ptr(),\n",
        "                                          c_float(cap), n, stream_ptr)\n",
        "        ctx.save_for_backward(x)\n",
        "        ctx.cap = cap\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, gy):\n",
        "        (x,) = ctx.saved_tensors\n",
        "        gy = gy.contiguous()\n",
        "        gx = torch.empty_like(x)\n",
        "        n = x.numel()\n",
        "        stream_ptr = torch.cuda.current_stream().cuda_stream\n",
        "\n",
        "        lib.clipped_relu_backward_launcher(x.data_ptr(), gy.data_ptr(), gx.data_ptr(),\n",
        "                                           c_float(ctx.cap), n, stream_ptr)\n",
        "        return gx, None          # None for cap (not learnable)\n",
        "\n",
        "# convenience wrapper\n",
        "def clipped_relu(x, cap=6.0):\n",
        "    return ClippedReLU.apply(x, float(cap))\n"
      ],
      "metadata": {
        "id": "mIHUsnuZ1Ehb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor\n",
        "x = torch.randn(2_000_000, device='cuda')\n",
        "\n",
        "# correctness\n",
        "y_custom = clipped_relu(x, cap=6.0)\n",
        "y_ref    = torch.clamp(x, 0.0, 6.0)\n",
        "print(\"match:\", torch.allclose(y_custom, y_ref, atol=1e-6))\n",
        "\n",
        "# timing\n",
        "torch.cuda.synchronize()\n",
        "for _ in range(5): clipped_relu(x); torch.clamp(x, 0.0, 6.0)  # warm‑up\n",
        "\n",
        "start = torch.cuda.Event(True); end = torch.cuda.Event(True)\n",
        "\n",
        "start.record(); clipped_relu(x); end.record(); torch.cuda.synchronize()\n",
        "print(\"custom CUDA:  %.3f ms\" % start.elapsed_time(end))\n",
        "\n",
        "start.record(); torch.clamp(x, 0.0, 6.0); end.record(); torch.cuda.synchronize()\n",
        "print(\"torch.clamp: %.3f ms\" % start.elapsed_time(end))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUy3YFXPyurJ",
        "outputId": "86e78c38-743e-4f87-f78e-34b8c586aa84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "match: True\n",
            "custom CUDA:  0.342 ms\n",
            "torch.clamp: 0.215 ms\n"
          ]
        }
      ]
    }
  ]
}